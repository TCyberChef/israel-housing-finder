---
phase: 03-yad2-scraper
plan: 04
type: execute
wave: 4
depends_on: ["03-03"]
files_modified:
  - src/scrapers/index.ts
  - src/scrapers/db/client.ts
  - src/scrapers/db/operations.ts
  - .github/workflows/scrape-yad2.yml
  - package.json
user_setup:
  - service: supabase
    why: "Database write access for scraper"
    env_vars:
      - name: SUPABASE_URL
        source: "Supabase Dashboard -> Project Settings -> API -> Project URL"
      - name: SUPABASE_SERVICE_KEY
        source: "Supabase Dashboard -> Project Settings -> API -> service_role secret (NOT anon key)"
    dashboard_config:
      - task: "Add GitHub Secrets for scraper workflow"
        location: "GitHub repo -> Settings -> Secrets and variables -> Actions -> New repository secret"
        details: "Add SUPABASE_URL and SUPABASE_SERVICE_KEY secrets"
autonomous: true

must_haves:
  truths:
    - "GitHub Actions workflow runs scraper every 6 hours"
    - "Scraped listings deduplicate via Phase 2 dedupe_hashes table"
    - "Duplicate listings (same content_hash) update last_seen timestamp on existing listing"
    - "New listings insert into both listings and dedupe_hashes tables"
    - "Stale listings (7+ days old) are marked inactive via is_active=false"
    - "Source attribution uses sources JSONB array format from Phase 2 schema"
    - "Workflow completes without errors when secrets are configured"
  artifacts:
    - path: "src/scrapers/index.ts"
      provides: "Entry point for scraper execution"
      exports: ["main"]
      min_lines: 30
    - path: "src/scrapers/db/client.ts"
      provides: "Supabase client initialization"
      exports: ["getSupabaseClient"]
      min_lines: 10
    - path: "src/scrapers/db/operations.ts"
      provides: "Database operations using dedupe_hashes table and sources JSONB"
      exports: ["upsertListings", "markStaleListings"]
      min_lines: 60
    - path: ".github/workflows/scrape-yad2.yml"
      provides: "Scheduled GitHub Actions workflow"
      contains: "cron.*0 \\*/6"
    - path: "package.json"
      contains: "\"scrape\":"
  key_links:
    - from: "src/scrapers/index.ts"
      to: "src/scrapers/yad2.ts"
      via: "import scrapeYad2"
      pattern: "scrapeYad2\\(\\)"
    - from: "src/scrapers/index.ts"
      to: "src/scrapers/db/operations.ts"
      via: "upsertListings call"
      pattern: "upsertListings.*listings"
    - from: "src/scrapers/db/operations.ts"
      to: "dedupe_hashes table"
      via: "query content_hash for deduplication"
      pattern: "from\\('dedupe_hashes'\\)"
    - from: "src/scrapers/db/operations.ts"
      to: "sources JSONB"
      via: "convert flat fields to sources array"
      pattern: "sources:.*platform.*url.*scraped_at"
    - from: ".github/workflows/scrape-yad2.yml"
      to: "npm run scrape"
      via: "workflow run step"
      pattern: "npm run scrape"
---

<objective>
Integrate scraper with Supabase database and GitHub Actions scheduled workflow.

Purpose: Complete the data pipeline from Yad2 to database using Phase 2's dedupe_hashes table for deduplication, sources JSONB format, stale listing detection using is_active/last_seen, and scheduled execution every 6 hours.
Output: Working end-to-end scraper pipeline with GitHub Actions workflow, database operations using Phase 2 dedupe_hashes table, and monitoring.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-yad2-scraper/03-RESEARCH.md
@.planning/phases/03-yad2-scraper/03-01-SUMMARY.md
@.planning/phases/03-yad2-scraper/03-02-SUMMARY.md
@.planning/phases/03-yad2-scraper/03-03-SUMMARY.md
@src/scrapers/yad2.ts
@src/scrapers/types.ts
@src/scrapers/utils/hash.ts
@supabase/migrations/20260213010445_create_core_schema.sql
@supabase/migrations/20260213020000_add_scraper_fields.sql
@.github/workflows/deploy.yml
</context>

<tasks>

<task type="auto">
  <name>Create database client and operations</name>
  <files>src/scrapers/db/client.ts, src/scrapers/db/operations.ts</files>
  <action>
    Create two files:

    **1. src/scrapers/db/client.ts** - Supabase client initialization:
    - Import { createClient } from '@supabase/supabase-js'
    - Export function getSupabaseClient():
      - Read SUPABASE_URL and SUPABASE_SERVICE_KEY from process.env
      - Validate both are set (throw error if missing with helpful message)
      - Return createClient(url, serviceKey)
    - Use service_role key (not anon key) for write access

    **2. src/scrapers/db/operations.ts** - Database operations:

    Export async function upsertListings(listings: Listing[]): Promise<void>
    - Get Supabase client
    - For each listing:
      1. Generate content_hash using generateDedupeHash(address, rooms, size_sqm)
      2. Check if hash exists in dedupe_hashes table:
         - .from('dedupe_hashes').select('listing_id').eq('content_hash', hash).maybeSingle()
      3. If hash EXISTS (duplicate):
         - Update existing listing's mutable fields (price, photos, last_seen, is_active=true)
         - .from('listings').update({ price, photos, sources, last_seen: now, is_active: true }).eq('id', existing.listing_id)
      4. If hash NOT found (new listing):
         - Convert flat source fields to sources JSONB array: [{ platform: source_platform, url: source_url, scraped_at: timestamp }]
         - Insert new listing row: { address, city, price, rooms, size_sqm, floor, photos, sources, last_seen: now, is_active: true }
         - Get back the new listing id from .select('id')
         - Insert hash row: .from('dedupe_hashes').insert({ listing_id: newId, content_hash: hash })
      - Log success/error for each listing

    Export async function markStaleListings(): Promise<void>
    - Get Supabase client
    - Calculate cutoff: 7 days ago
    - Update query:
      - .from('listings')
      - .update({ is_active: false })
      - .lt('last_seen', cutoff)
      - .eq('is_active', true)
    - Log count of rows updated
    - Handle errors

    Import { log } from '../utils/logger' for logging
    Import { generateDedupeHash } from '../utils/hash'
    Import { Listing } from '../types'

    **Critical schema alignment:**
    - Phase 2 schema: dedupe_hashes table (content_hash + listing_id FK) - use for dedup lookups
    - Phase 2 schema: sources JSONB array (not flat source_url/source_platform/source_id)
    - Phase 2 schema: size_sqm (not size)
    - Phase 3 migration adds: is_active, last_seen (on listings table)
    - NO dedupe_hash column on listings table - dedup is via separate dedupe_hashes table
  </action>
  <verify>npx tsc --project tsconfig.scraper.json --noEmit</verify>
  <done>Database client and operations exist, implement upsert with dedupe_hash deduplication and sources JSONB conversion, stale marking uses is_active</done>
</task>

<task type="auto">
  <name>Create scraper entry point and npm script</name>
  <files>src/scrapers/index.ts, package.json</files>
  <action>
    **1. src/scrapers/index.ts** - Main execution entry point:
    - Import { scrapeYad2 } from './yad2'
    - Import { upsertListings, markStaleListings } from './db/operations'
    - Import { log } from './utils/logger'

    Export async function main():
      - Log: "Starting Yad2 scraper..."
      - Call scrapeYad2()
      - Log: "Scraped {count} listings"
      - Call upsertListings(listings)
      - Log: "Listings saved to database"
      - Call markStaleListings('yad2')
      - Log: "Stale listings marked inactive"
      - Log: "Scraper completed successfully"

    Error handling:
      - Wrap in try/catch
      - Log errors with full context
      - process.exit(1) on error (for GitHub Actions failure detection)
      - process.exit(0) on success

    Execute: if (require.main === module) { main() }

    **2. package.json** - Add scrape script:
    - Add to scripts: "scrape": "tsx src/scrapers/index.ts"
    - This allows: npm run scrape (for GitHub Actions and local testing)
  </action>
  <verify>
    npx tsc --project tsconfig.scraper.json --noEmit
    (Manual test with secrets: SUPABASE_URL=xxx SUPABASE_SERVICE_KEY=xxx npm run scrape - but don't run in automated verification without secrets)
  </verify>
  <done>Entry point exists, integrates scraper + database operations, npm run scrape works</done>
</task>

<task type="auto">
  <name>Create GitHub Actions scheduled workflow</name>
  <files>.github/workflows/scrape-yad2.yml</files>
  <action>
    Create .github/workflows/scrape-yad2.yml:

    **Trigger:**
    - schedule: cron: '0 */6 * * *' (every 6 hours at minute 0)
    - workflow_dispatch: {} (manual trigger for testing)

    **Job: scrape**
    - runs-on: ubuntu-latest
    - timeout-minutes: 30 (prevent hung workflows)

    **Steps:**
    1. Checkout repository (actions/checkout@v4)
    2. Setup Node.js 20 (actions/setup-node@v4)
    3. Install dependencies (npm ci)
    4. Run scraper (npm run scrape)
       - env:
         - SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
         - SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}

    **Note:** Workflow will fail until user adds GitHub Secrets (SUPABASE_URL, SUPABASE_SERVICE_KEY) in repo settings. Document this requirement in SUMMARY.

    Reference existing deploy.yml for action versions and structure.
  </action>
  <verify>
    .github/workflows/scrape-yad2.yml exists
    grep -q "cron.*0 \\*/6" .github/workflows/scrape-yad2.yml
    grep -q "npm run scrape" .github/workflows/scrape-yad2.yml
  </verify>
  <done>GitHub Actions workflow exists with 6-hour schedule, manual trigger, and Supabase secrets integration</done>
</task>

</tasks>

<verification>
- [ ] src/scrapers/db/client.ts initializes Supabase with service_role key
- [ ] src/scrapers/db/operations.ts checks dedupe_hashes table for existing content_hash
- [ ] src/scrapers/db/operations.ts updates existing listing when hash found (last_seen, price, etc.)
- [ ] src/scrapers/db/operations.ts inserts new listing + dedupe_hashes row when hash not found
- [ ] src/scrapers/db/operations.ts converts flat source fields to sources JSONB array
- [ ] src/scrapers/db/operations.ts uses size_sqm field (not size)
- [ ] src/scrapers/db/operations.ts implements markStaleListings using is_active and last_seen
- [ ] src/scrapers/index.ts integrates scraper + database operations
- [ ] package.json includes "scrape" script using tsx
- [ ] .github/workflows/scrape-yad2.yml has cron schedule (0 */6 * * *)
- [ ] Workflow uses SUPABASE_URL and SUPABASE_SERVICE_KEY secrets
- [ ] Workflow has workflow_dispatch for manual testing
- [ ] TypeScript compiles all scraper code without errors
</verification>

<success_criteria>
End-to-end scraper pipeline is complete:
- Database client initializes with service_role key from env vars
- Deduplication uses Phase 2's dedupe_hashes table (content_hash + listing_id FK)
- Duplicate listings update last_seen and mutable fields on existing listing row
- New listings insert into both listings and dedupe_hashes tables
- Source fields converted to sources JSONB array format matching Phase 2 schema
- Stale listings (not seen in 7 days) are marked inactive using is_active column
- Entry point integrates scraping, upserting, and stale marking
- GitHub Actions workflow runs scraper every 6 hours
- Workflow supports manual trigger for testing
- npm run scrape command works for local execution
</success_criteria>

<output>
After completion, create `.planning/phases/03-yad2-scraper/03-04-SUMMARY.md`
</output>
